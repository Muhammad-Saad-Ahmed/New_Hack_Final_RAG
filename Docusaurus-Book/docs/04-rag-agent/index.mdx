--- 
title: RAG Agent
---

# RAG Agent

This document outlines the implementation of the RAG (Retrieval-Augmented Generation) agent, which leverages FastAPI for its API endpoints and integrates with Cohere for embeddings and Qdrant for vector database operations.

```python
import os
import re
import uuid
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import cohere
import qdrant_client
from qdrant_client.http import models

# --- CONFIGURATION ---
# Load environment variables from .env file
# This ensures that API keys and other configurations are loaded securely.
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '.env'))
TARGET_SITEMAP_URL = "https://new-hack-final-rag-kvxh.vercel.app/sitemap.xml"
COLLECTION_NAME = "New-Rag-Data"

# Initialize Clients
# The Cohere client is used to generate embeddings for text.
co = cohere.Client(os.getenv("COHERE_API_KEY"))
# The Qdrant client connects to the vector database where document embeddings are stored.
qdrant_db_client = qdrant_client.QdrantClient(
    url=os.getenv("QDRANT_URL"), 
    api_key=os.getenv("QDRANT_API_KEY")
)

# --- FastAPI App Initialization ---
app = FastAPI()

# --- Pydantic Models ---
class IngestionSummary(BaseModel):
    """Summary of the data ingestion process."""
    total_urls_found: int
    processed_urls: int
    total_text_chunks_created: int
    total_embeddings_generated: int
    message: str

# --- FUNCTIONS (from existing ingestion pipeline) ---
def get_all_urls(sitemap_url: str) -> list[str]:
    """Fetches and parses a sitemap to extract all URLs."""
    print(f"Fetching URLs from {sitemap_url}...")
    urls = []
    try:
        response = requests.get(sitemap_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'lxml-xml')
        locs = soup.find_all('loc')
        urls = [loc.text for loc in locs]
        print(f"Found {len(urls)} URLs.")
    except requests.exceptions.RequestException as e:
        print(f"Error fetching sitemap: {e}")
    return urls

def extract_text_from_url(url: str) -> tuple[str, str]:
    """Extracts the main text content and title from a URL."""
    print(f"  Extracting text from {url}...")
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Docusaurus-specific selectors - may need adjustment for other sites
        main_content = soup.find('main') or soup.find('article')
        if not main_content:
            return "", ""

        # Remove irrelevant elements
        for element in main_content.find_all(['nav', 'footer', 'aside', 'script', 'style']):
            element.decompose()
        
        text = main_content.get_text(separator=' ', strip=True)
        title = soup.title.string if soup.title else "No Title"
        
        # Clean up excessive whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text, title
    except requests.exceptions.RequestException as e:
        print(f"    Error fetching URL {url}: {e}")
        return "", ""

def chunk_text(text: str, chunk_size: int = 512, overlap: int = 64) -> list[str]:
    """Splits text into fixed-size chunks with overlap."""
    if not text:
        return []
    # Using a simple token approximation (split by space)
    tokens = text.split()
    chunks = []
    for i in range(0, len(tokens), chunk_size - overlap):
        chunk = " ".join(tokens[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

def embed_chunks(chunks: list[str]) -> list[list[float]]:
    """Generates embeddings for a list of text chunks using Cohere."""
    if not chunks:
        return []
    print(f"    Generating {len(chunks)} embeddings...")
    try:
        response = co.embed(
            texts=chunks,
            model='embed-english-v3.0',
            input_type='search_document'
        )
        return response.embeddings
    except cohere.CohereAPIError as e: # Corrected error class
        print(f"    Cohere API error: {e}")
        return []

def create_collection(collection_name: str):
    """Creates a Qdrant collection if it doesn't exist."""
    print(f"Checking/Creating Qdrant collection '{collection_name}'...")
    try:
        qdrant_db_client.recreate_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE)
        )
        print("  Collection created successfully.")
    except Exception as e:
        # A more robust check would be to get collection info and handle specific errors
        print(f"  Collection likely already exists. Error: {e}")


def save_chunks_to_qdrant(collection_name: str, chunks: list[str], embeddings: list[list[float]], source_url: str, page_title: str):
    """Saves text chunks and their embeddings to Qdrant."""
    if not chunks or not embeddings:
        return
    print(f"    Saving {len(chunks)} chunks to Qdrant...")
    points = []
    for chunk, embedding in zip(chunks, embeddings):
        points.append(models.PointStruct(
            id=str(uuid.uuid4()),
            vector=embedding,
            payload={
                "text": chunk,
                "source_url": source_url,
                "page_title": page_title,
                "section_title": "" # Placeholder - more advanced parsing needed for this
            }
        ))

    if points:
        qdrant_db_client.upsert(collection_name=collection_name, points=points, wait=True)
        print("    Successfully saved chunks.")

# Import retrieval functions
from .retrieving import retrieve_chunks # Adjusted import for relative path

# --- FastAPI App Initialization ---
app = FastAPI()

# --- Pydantic Models ---
class IngestionSummary(BaseModel):
    """Summary of the data ingestion process."""
    total_urls_found: int
    processed_urls: int
    total_text_chunks_created: int
    total_embeddings_generated: int
    message: str

class QueryRequest(BaseModel):
    """Request model for a search query."""
    query: str
    top_k: int = 5 # Default to 5 relevant results

class RetrievedChunk(BaseModel):
    """Represents a single retrieved chunk of information."""
    id: str
    score: float
    payload: dict # Assuming payload can be any dictionary, refine if specific keys are always present

class RetrievalResponse(BaseModel):
    """Response model for retrieval results."""
    query: str
    results: list[RetrievedChunk]
    total_results: int

# --- API Endpoints ---
@app.get("/")
async def read_root():
    return {"message": "RAG FastAPI is running!"}

@app.post("/retrieve", response_model=RetrievalResponse)
async def retrieve(request: QueryRequest):
    """
    Retrieves relevant information from the Qdrant database based on a user query.
    """
    print(f"Retrieving for query: '{request.query}' with top_k: {request.top_k}")
    retrieved_data = retrieve_chunks(request.query, co, qdrant_db_client, top_k=request.top_k)
    
    # Convert retrieved_data to Pydantic models
    formatted_results = [RetrievedChunk(**chunk) for chunk in retrieved_data]

    return RetrievalResponse(
        query=request.query,
        results=formatted_results,
        total_results=len(formatted_results)
    )

@app.post("/ingest", response_model=IngestionSummary)
async def ingest_data():
    """
    Triggers the RAG ingestion pipeline to fetch data from the sitemap,
    extract text, chunk it, embed it, and save to Qdrant.
    """
    print("--- Starting RAG Ingestion Pipeline ---")
    
    # 1. Create Qdrant collection
    create_collection(COLLECTION_NAME)
    
    # 2. Get all URLs from the sitemap
    urls = get_all_urls(TARGET_SITEMAP_URL)
    url_count = len(urls)
    if not urls:
        raise HTTPException(status_code=500, detail="No URLs found from sitemap for ingestion.")

    total_chunks = 0
    total_embeddings = 0
    processed_urls = 0

    # 3. Process each URL
    for url in urls:
        full_text, page_title = extract_text_from_url(url)
        if not full_text:
            continue
        
        processed_urls += 1
        text_chunks = chunk_text(full_text)
        if not text_chunks:
            continue
        
        num_chunks = len(text_chunks)
        total_chunks += num_chunks

        embeddings = embed_chunks(text_chunks)
        if not embeddings:
            continue
        
        num_embeddings = len(embeddings)
        total_embeddings += num_embeddings

        save_chunks_to_qdrant(COLLECTION_NAME, text_chunks, embeddings, url, page_title)

    print("--- RAG Ingestion Pipeline Finished ---")
    summary_message = "All data has been embedded and saved into the '{COLLECTION_NAME}' vector database."
    print("\n--- Summary ---")
    print(f"Total URLs found: {url_count}")
    print(f"Processed URLs: {processed_urls}")
    print(f"Total text chunks created: {total_chunks}")
    print(f"Total embeddings generated: {total_embeddings}")
    print(summary_message)
    print("---------------")

    return IngestionSummary(
        total_urls_found=url_count,
        processed_urls=processed_urls,
        total_text_chunks_created=total_chunks,
        total_embeddings_generated=total_embeddings,
        message=summary_message
    )
```